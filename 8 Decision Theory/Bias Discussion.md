The Invisible Bug

The prompt poses an interesting dichotomy between models, specifically linear models, and human judgement. Annie Nguyen made an excellent post pointing out that these concepts often blur, and we should also consider how judgement affects the development of models. There is a lot of pressure to sell algorithms as objective, quite literally in the case of entire companies built around marketing their industry-specific models. Even in popular culture, teams like the Oakland A's and the Houston Astros* have popularized the concept that forward-thinking analytics can outperforming experienced human judgement with unbiased decisions. This class doesn't need a primer on the fact that models are useful, and frequently more so than a human attempting to solve the same problem. That is not, however, equivalent to them being unbiased. As Cathy O'Neil points out in her book, "Weapons of Math Destruction, "A model's blind spots reflect the judgement and priorities of its creators." 

Even in the most advanced machine learning algorithms, several decisions have to be made by humans that each introduce an opportunity for bias. This term carries some political and cultural baggage in the U.S., and it's important that we understand bias as gaps in perspective that affect us all. The most capable and well-intentioned data workers will still introduce bias into their work, and there probably isn't an algorithmic solution to fixing humans on the horizon. Rather, we can recognize the shortcomings of our lived experiences and plan for them the same way that we plan for scaling code from our personal machines to production systems. 

The common solution offered for addressing bias is diversity in the development community, and it is true that the data industry needs to improve along these lines. However, algorithms developed by any decently large organization will likely impact more people than you can realistically represent on a team and judgement errors will always slip through. Instead, I propose that we should rethink QA processes to include continuous testing for bias. This would entail not just measure the performance of our models, but examining the ramifications of the decisions they drive. If negative externalities are identified, it may not be obvious how or if it can be addressed at a reasonable cost. Fortunately, problem solving is a necessary skill for this trade and the alternative is pretending we can calculate all effects at the outset and closing our eyes once an algorithm is unleashed on the world.

Conceptually, this isn't much different from the processes of iterative development that would be familiar to most developers.  There are entire systems in place for processing and troubleshooting the unforseeable consequences of our software, which we are more accustomed to calling "bugs." Of all professions, people experienced with development should have the humility to understand that we frequently don't get things right  on the first or sometimes 80th attempt. As the products we create expand in scope and influence, it will be increasingly destructive to believe that our responsibility ends at prediction and functionality. If our proclivities seep into our models and result in harmful outcomes, O'Neil warns us that we may be left asking if "we've eliminated human bias, or simply camouflaged it with technology."