# 6.0 Graphical ETL in Pentaho
## Overview
### Transformations
- Two levels of ETL: Logical and Data flow 
- Transformations are at the data flow level
	- Moving data from point A to point B
	- Also for cleaning and massaging data
- GUI based programming
	- Limited by settings allowed 
	- Common controls
		- CSV file input
		- Text file input and output
		- Mostly any I/O 
### Transformations in Pentaho
#### CSV Input
- Input -> CSV file input
	- Drag into palette 
- Change Step name to desired name
- Filename
	- Browse to file and insert
		- Most of the time you want to use variables
	- Variables
		- In the filename field, press ctrl + space to get a list of variables to insert
		- The one we want is `${Internal.Entry.Current.Directory}` which replaces our current directory in the file path
		- Allows for easier sharing, only the relative path will need to be filled in
- Delimiter
	- Double check, in our case we use the pipe (|)
- Enclosure
	- What symbol is used to escape the delimiter character
	- Not needed in our case
- Press Get Fields to get them!
	- Double check Type fields because Pentaho is guessing them
/Images/6.1.1_CSV_Input.png
#### Output
- Output -> Text file output
- Hover over input, click the arrow coming out of box (highlighted in output below) and drag the arrow to the output
	- Select main output of step
/Images/6.1.2_Connection.png
- Text File Output
	- Filename: Use variable for path, then name output file
	- Don't name it to csv
		- Instead change Extension field to csv
		- Otherwise we might get .csv.csv
	- Content tab
		- Separator = delimiter, change to our delimiter
		- Format: End of line terminators
			- We use CR + LF
	- Fields tab
		- Shows fields, which can be rearranged
- Now we can run the ETL and should get our output file we named above
### Jobs
- Jobs are the logical flows of the ETL
- Setting variables
	- Matters a lot for production environments
	- Paths, credentials, etc
- Ordering transformations
- Calling outside functions
	- Python functions, calls to websites, etc
- Higher conditional function
	- “File exists” checks
### Jobs in Pentaho
#### Transformation Parameters
- Found in View tab in top left
	- Right click the transformation
	- Move to parameters tab
- Parameters are global to the entire transformation
	- But steps in transformation can change parameters
- Create a parameter for our transformation path
	- Parameter: datapath
	- Default value: \data\
- We can insert the datapath variable after the local path variable we created above
	- Combined these create the full path for our data location
	- Useful because different slashes are used between OS's
#### Add a Job
- Create new -> Job
	- Save the file
- Under General drag out Start and Success
	- In between, drag out transformation
	- Connect start to transformation
- Create datapath parameter in job
	- View -> right click job, same steps as the transformation parameter
- Transformation
	- Don't browse, use the CD variable and our datapath parameter
		- You may need to click get parameters at the bottom of the tab
	- Connect transformation to success
- Run the job
#### Add IfExists
- Job checks to see if file exists
- Search for file exists under job design tab
- Delete the connection from start to transformation
	- Replace with connection from start to file exists to transformation
- File exists
	- Just add the file location with variables because we follow good practices
	- CD, datapath, sales_data.csv
		- sales_data is the file we want to confirm exists
/Images/6.1.3_Job.png
#### Logs
- Job metrics tab is helpful to review the job process
	- It doesn't give us transformation info
- Transformation execution results
	- Preview data is useful to compare different files in the transformation to look for expected results like added/removed columns
## Pentaho Data Munging 
### Munging the Stream
- Similar techniques as Python
	- Just need to know which controls do what
- Most controls work at row level, not set data
	- Vs Pandas which works on entire dataframe
#### Munging Controls
- Calculator
	- Allows you to create new fields with calculations
	- Field labels are the other fields used in the calculation 
	- Date data types can have elements of date extracted by calculation
	- Warning: if you multiple int by decimal, make sure decimal comes first 
		- or change data types
	/Images/6.2.1_Calculator.png
- Value Mapper
	- Used to map values to new values, either in place or in a new column
	- Inputs
		- Field you're using
		- Target field for new values
			- If left empty, values are overwritten on current field
		- Default input if there is no match
	- Field values
		- Source value
			- Value to be mapped
		- Target value
			- Value to map
	/Images/6.2.2_Value_Mapper.png 
- String Cut
	- Cut string in one field, output result to another
	/Images/6.2.3_StringCut.png
/Images/6.2.3_Munge_Transformation.png
### Custom Coding
- Available in Pentaho
	- In JavaScript
	- Can be done in line of control
Example script to check if value is even/odd:
/Images/6.2.3_CustomJS.png
### Flow Paths and Error Handling
- We can do parallel processes instead of just serial
#### Multicast vs. conditional pathing
- Multicast: every row is coming in to every path coming out of a control
- Conditional: path filters rows 
- How these are merged differs
	- Multicast uses merge join
	- Conditional uses union
#### Pathway Controls
/Images/6.2.4_Paths.png
- Error controls
	- Push offending rows into a new 'error' path
	- Edit and remerge into data
	- Can also write errors to log
- Distribute rows
	- Distributes rows to several paths
	- Especially useful if you're running on separate nodes
	- Similar to load balancing
- Merge Join
	- Select order, type of joins, and insert keys
	- Left: step 1, right: step 2
	- **Must be sorted by key before joining**
		- For Pentaho reasons, make sure both datasets are sorted by ascending keys
		- This is why we see sort rows control before every merge
		/Images/6.2.7_Merge_Sorts.png
	/Images/6.2.5_Merge_Join.png
- Filter Rows
	/Images/6.2.6_Filter_Rows.png
## Python in Pentaho
- We can call Python in transformation in job, we'll use a job
- Job -> Scripting -> Shell
	- Pentaho is looking for a .bat or .sh script
	- We need to write a short script that runs the python file
		- `.../python file.py` save as whatever.sh (in Unix)
- Script is executed and files are updated/created 
## JSON in Pentaho
- Convert JSON to CSV
- Job -> JSON input
	- Fields need to be specified for JSON, there's no 'get fields'
	- Use JSON Path Notation for 'Path' Field
		- Based on level of hierarchy
		- $.key.key...
		- Use * for all keys at the level of hierarchy 
		- E.g. $.Patients.*.Name
- To dive further, add a second JSON input to job
	- Check 'source is from previous step'
	- Select field that was defined in previous step that we want to drill into
	- Now when we define fields, the drill down field is our new 'root' ($)